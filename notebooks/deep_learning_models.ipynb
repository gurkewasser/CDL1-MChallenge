{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(search_from=\".\", indicator=\".git\")\n",
    "\n",
    "DATA_DIR = root / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_data_path = DATA_DIR / \"preproc\" / \"dl_data.npz\"\n",
    "dl_data = np.load(dl_data_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '/Users/arian/Documents/FHNW/cdl1/CDL1-MChallenge/data/preproc/dl_data.npz' with keys: X, y\n",
      "Keys: ['X', 'y']\n",
      "X: shape=(4129, 250, 20), dtype=float32\n",
      "y: shape=(4129,), dtype=<U8\n"
     ]
    }
   ],
   "source": [
    "print(dl_data)\n",
    "print(\"Keys:\", dl_data.files)\n",
    "for key in dl_data.files:\n",
    "    print(f\"{key}: shape={dl_data[key].shape}, dtype={dl_data[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n",
    "LSTM\n",
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.str_('climbing'): np.int64(0), np.str_('joggen'): np.int64(1), np.str_('sitting'): np.int64(2), np.str_('walking'): np.int64(3)}\n",
      "Epoch 1/20 - Train loss: 0.7310 - Val loss: 0.4699 - Val acc: 0.8498\n",
      "Epoch 2/20 - Train loss: 0.4027 - Val loss: 0.3471 - Val acc: 0.8691\n",
      "Epoch 3/20 - Train loss: 0.3421 - Val loss: 0.3066 - Val acc: 0.8740\n",
      "Epoch 4/20 - Train loss: 0.3076 - Val loss: 0.2901 - Val acc: 0.8756\n",
      "Epoch 5/20 - Train loss: 0.2905 - Val loss: 0.2771 - Val acc: 0.8998\n",
      "Epoch 6/20 - Train loss: 0.2769 - Val loss: 0.2598 - Val acc: 0.8885\n",
      "Epoch 7/20 - Train loss: 0.2578 - Val loss: 0.2526 - Val acc: 0.9063\n",
      "Epoch 8/20 - Train loss: 0.2558 - Val loss: 0.2439 - Val acc: 0.9079\n",
      "Epoch 9/20 - Train loss: 0.2345 - Val loss: 0.2349 - Val acc: 0.9079\n",
      "Epoch 10/20 - Train loss: 0.2248 - Val loss: 0.2129 - Val acc: 0.9160\n",
      "Epoch 11/20 - Train loss: 0.2037 - Val loss: 0.2145 - Val acc: 0.9144\n",
      "Epoch 12/20 - Train loss: 0.1950 - Val loss: 0.2074 - Val acc: 0.9225\n",
      "Epoch 13/20 - Train loss: 0.1780 - Val loss: 0.1876 - Val acc: 0.9241\n",
      "Epoch 14/20 - Train loss: 0.1767 - Val loss: 0.1712 - Val acc: 0.9386\n",
      "Epoch 15/20 - Train loss: 0.1515 - Val loss: 0.1636 - Val acc: 0.9418\n",
      "Epoch 16/20 - Train loss: 0.1226 - Val loss: 0.1258 - Val acc: 0.9532\n",
      "Epoch 17/20 - Train loss: 0.1019 - Val loss: 0.0960 - Val acc: 0.9661\n",
      "Epoch 18/20 - Train loss: 0.0833 - Val loss: 0.0897 - Val acc: 0.9677\n",
      "Epoch 19/20 - Train loss: 0.0978 - Val loss: 0.1320 - Val acc: 0.9483\n",
      "Epoch 20/20 - Train loss: 0.0941 - Val loss: 0.0926 - Val acc: 0.9693\n",
      "Test Accuracy: 0.9758\n",
      "Test Precision: 0.9762\n",
      "Test Recall: 0.9758\n",
      "Test F1-score: 0.9754\n",
      "Example true labels: ['climbing' 'climbing' 'joggen' 'climbing' 'climbing' 'sitting' 'climbing'\n",
      " 'climbing' 'climbing' 'climbing']\n",
      "Example predicted labels: ['climbing' 'climbing' 'joggen' 'climbing' 'climbing' 'sitting' 'climbing'\n",
      " 'climbing' 'climbing' 'climbing']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X = dl_data['X']  # shape: (N, T, C) or (N, C, T)\n",
    "y = dl_data['y']  # shape: (N,)\n",
    "\n",
    "# If y is object dtype or string, encode to integer labels\n",
    "if y.dtype.kind in {'U', 'S', 'O'} or not np.issubdtype(y.dtype, np.integer):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "else:\n",
    "    le = None  # No label encoding needed\n",
    "\n",
    "if X.shape[1] < X.shape[2]:  # (N, T, C) -> (N, C, T)\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "# Split into train, val, test (e.g., 70/15/15)\n",
    "N = X.shape[0]\n",
    "n_train = int(0.7 * N)\n",
    "n_val = int(0.15 * N)\n",
    "n_test = N - n_train - n_val\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_set, val_set, test_set = random_split(dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "# Define a simple 1D CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(in_channels=X.shape[1], num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                preds = out.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.cpu().numpy())\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(np.concatenate(all_labels), np.concatenate(all_preds))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=20)\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(yb.cpu().numpy())\n",
    "y_true = np.concatenate(all_labels)\n",
    "y_pred = np.concatenate(all_preds)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test Precision: {prec:.4f}\")\n",
    "print(f\"Test Recall: {rec:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "# If you want to map predictions back to string labels:\n",
    "if le is not None:\n",
    "    y_true_labels = le.inverse_transform(y_true)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "    print(\"Example true labels:\", y_true_labels[:10])\n",
    "    print(\"Example predicted labels:\", y_pred_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
