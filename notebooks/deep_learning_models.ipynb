{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(search_from=\".\", indicator=\".git\")\n",
    "\n",
    "DATA_DIR_TRAIN = root / \"data\" / \"DL\" / \"TRAIN\"\n",
    "DATA_DIR_TEST = root / \"data\" / \"DL\" / \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_data_path_train = DATA_DIR_TRAIN / \"dl_data_train.npz\"\n",
    "dl_data_path_test = DATA_DIR_TEST / \"dl_data_test.npz\"\n",
    "dl_data_train = np.load(dl_data_path_train, allow_pickle=True)\n",
    "dl_data_test = np.load(dl_data_path_test, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '/home/arian/cdl1/CDL1-MChallenge/data/DL/TRAIN/dl_data_train.npz' with keys: X, y\n",
      "Keys: ['X', 'y']\n",
      "X: shape=(3699, 250, 20), dtype=float32\n",
      "y: shape=(3699,), dtype=object\n",
      "NpzFile '/home/arian/cdl1/CDL1-MChallenge/data/DL/TEST/dl_data_test.npz' with keys: X, y\n",
      "Keys: ['X', 'y']\n",
      "X: shape=(1253, 250, 20), dtype=float32\n",
      "y: shape=(1253,), dtype=object\n"
     ]
    }
   ],
   "source": [
    "print(dl_data_train)\n",
    "print(\"Keys:\", dl_data_train.files)\n",
    "for key in dl_data_train.files:\n",
    "    print(f\"{key}: shape={dl_data_train[key].shape}, dtype={dl_data_train[key].dtype}\")\n",
    "\n",
    "print(dl_data_test)\n",
    "print(\"Keys:\", dl_data_test.files)\n",
    "for key in dl_data_test.files:\n",
    "    print(f\"{key}: shape={dl_data_test[key].shape}, dtype={dl_data_test[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n",
    "LSTM\n",
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'climbing': np.int64(0), 'joggen': np.int64(1), 'sitting': np.int64(2), 'walking': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X = dl_data_train['X']  # shape: (N, T, C) or (N, C, T)\n",
    "y = dl_data_train['y']  # shape: (N,)\n",
    "\n",
    "# If y is object dtype or string, encode to integer labels\n",
    "if y.dtype.kind in {'U', 'S', 'O'} or not np.issubdtype(y.dtype, np.integer):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "else:\n",
    "    le = None  # No label encoding needed\n",
    "\n",
    "if X.shape[1] < X.shape[2]:  # (N, T, C) -> (N, C, T)\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "# Split into train, val, test (e.g., 80/10/10)\n",
    "N = X.shape[0]\n",
    "n_train = int(0.8 * N)\n",
    "n_val = int(0.1 * N)\n",
    "n_test = N - n_train - n_val\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_set, val_set, test_set = random_split(dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, 2, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.squeeze(-1)  # (B, 64)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNNBaseline(in_channels=X.shape[1], num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marian-iseni\u001b[0m (\u001b[33marianarian\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../data/log/wandb/run-20250608_192027-1v1u92zx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arianarian/cdl1/runs/1v1u92zx' target=\"_blank\">simple-cnn-baseline</a></strong> to <a href='https://wandb.ai/arianarian/cdl1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arianarian/cdl1' target=\"_blank\">https://wandb.ai/arianarian/cdl1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arianarian/cdl1/runs/1v1u92zx' target=\"_blank\">https://wandb.ai/arianarian/cdl1/runs/1v1u92zx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arianarian/cdl1/runs/1v1u92zx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7271403edab0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(\"../data/log\", exist_ok=True)\n",
    "\n",
    "# Initialize wandb with custom log directory\n",
    "wandb.init(\n",
    "    project=\"cdl1\",\n",
    "    name=\"simple-cnn-baseline\",\n",
    "    config={\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"model\": \"SimpleCNN\",\n",
    "        \"in_channels\": X.shape[1],\n",
    "        \"num_classes\": num_classes,\n",
    "    },\n",
    "    dir=\"../data/log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with wandb logging\n",
    "def train_model(model, train_loader, val_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                preds = out.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(yb.cpu().numpy())\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(np.concatenate(all_labels), np.concatenate(all_preds))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train loss: 0.6262 - Val loss: 0.1857 - Val acc: 0.9079\n",
      "Epoch 2/10 - Train loss: 0.2260 - Val loss: 0.1455 - Val acc: 0.9458\n",
      "Epoch 3/10 - Train loss: 0.1715 - Val loss: 0.1209 - Val acc: 0.9539\n",
      "Epoch 4/10 - Train loss: 0.1798 - Val loss: 0.2257 - Val acc: 0.9079\n",
      "Epoch 5/10 - Train loss: 0.1913 - Val loss: 0.1085 - Val acc: 0.9539\n",
      "Epoch 6/10 - Train loss: 0.1261 - Val loss: 0.1163 - Val acc: 0.9485\n",
      "Epoch 7/10 - Train loss: 0.1287 - Val loss: 0.1318 - Val acc: 0.9458\n",
      "Epoch 8/10 - Train loss: 0.1063 - Val loss: 0.1354 - Val acc: 0.9214\n",
      "Epoch 9/10 - Train loss: 0.1032 - Val loss: 0.1254 - Val acc: 0.9702\n",
      "Epoch 10/10 - Train loss: 0.0811 - Val loss: 0.1550 - Val acc: 0.9133\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8949\n",
      "Test Precision: 0.9056\n",
      "Test Recall: 0.8949\n",
      "Test F1-score: 0.8886\n",
      "Example true labels: ['sitting' 'walking' 'joggen' 'sitting' 'climbing' 'joggen' 'walking'\n",
      " 'climbing' 'sitting' 'walking']\n",
      "Example predicted labels: ['sitting' 'climbing' 'joggen' 'sitting' 'climbing' 'joggen' 'walking'\n",
      " 'climbing' 'sitting' 'walking']\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆▁▆▆▅▃█▂</td></tr><tr><td>val_loss</td><td>▆▃▂█▁▁▂▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.89488</td></tr><tr><td>test_f1</td><td>0.88859</td></tr><tr><td>test_precision</td><td>0.90563</td></tr><tr><td>test_recall</td><td>0.89488</td></tr><tr><td>train_loss</td><td>0.08112</td></tr><tr><td>val_acc</td><td>0.91328</td></tr><tr><td>val_loss</td><td>0.15498</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">simple-cnn-baseline</strong> at: <a href='https://wandb.ai/arianarian/cdl1/runs/1v1u92zx' target=\"_blank\">https://wandb.ai/arianarian/cdl1/runs/1v1u92zx</a><br> View project at: <a href='https://wandb.ai/arianarian/cdl1' target=\"_blank\">https://wandb.ai/arianarian/cdl1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../data/log/wandb/run-20250608_192027-1v1u92zx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(yb.cpu().numpy())\n",
    "y_true = np.concatenate(all_labels)\n",
    "y_pred = np.concatenate(all_preds)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test Precision: {prec:.4f}\")\n",
    "print(f\"Test Recall: {rec:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "# Log test metrics to wandb\n",
    "wandb.log({\n",
    "    \"test_accuracy\": acc,\n",
    "    \"test_precision\": prec,\n",
    "    \"test_recall\": rec,\n",
    "    \"test_f1\": f1,\n",
    "})\n",
    "\n",
    "# If you want to map predictions back to string labels:\n",
    "if le is not None:\n",
    "    y_true_labels = le.inverse_transform(y_true)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "    print(\"Example true labels:\", y_true_labels[:10])\n",
    "    print(\"Example predicted labels:\", y_pred_labels[:10])\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP and pynndescent are currently incompatible with numpy>=2.0 due to use of np.infty.\n",
    "# As a workaround, use PCA for visualization instead of UMAP.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Helper to get features from the penultimate layer\n",
    "def get_penultimate_features(model, xb):\n",
    "    \"\"\"\n",
    "    Try to extract features from the penultimate layer of the model.\n",
    "    This function handles common model architectures.\n",
    "    \"\"\"\n",
    "    # If model has a feature extractor, use it\n",
    "    if hasattr(model, 'features'):\n",
    "        feats = model.features(xb)\n",
    "    elif hasattr(model, 'extract_features'):\n",
    "        feats = model.extract_features(xb)\n",
    "    else:\n",
    "        # Try to find the last nn.Linear layer before the classifier\n",
    "        # This works for models with a 'classifier' or 'fc' attribute that is nn.Sequential or nn.ModuleList\n",
    "        if hasattr(model, 'classifier') and isinstance(model.classifier, torch.nn.Sequential):\n",
    "            modules = list(model.classifier.children())\n",
    "            if len(modules) > 1:\n",
    "                penultimate = torch.nn.Sequential(*modules[:-1])\n",
    "                feats = penultimate(xb)\n",
    "            else:\n",
    "                feats = xb\n",
    "        elif hasattr(model, 'fc') and isinstance(model.fc, torch.nn.Sequential):\n",
    "            modules = list(model.fc.children())\n",
    "            if len(modules) > 1:\n",
    "                penultimate = torch.nn.Sequential(*modules[:-1])\n",
    "                feats = penultimate(xb)\n",
    "            else:\n",
    "                feats = xb\n",
    "        elif hasattr(model, 'fc') and isinstance(model.fc, torch.nn.Linear):\n",
    "            # Instead of calling model.fc(xb), call the model up to the penultimate layer\n",
    "            # Try to find the layer before 'fc'\n",
    "            # This is a best-effort fallback: run the model up to the last layer\n",
    "            # If model has a 'forward_features' method, use it\n",
    "            if hasattr(model, 'forward_features'):\n",
    "                feats = model.forward_features(xb)\n",
    "            else:\n",
    "                # As a last resort, remove the last layer from model._modules if possible\n",
    "                # This is fragile and may not work for all models\n",
    "                # So just use the input\n",
    "                feats = xb\n",
    "        else:\n",
    "            feats = xb  # fallback to input\n",
    "    return feats\n",
    "\n",
    "# Get the features from the train set\n",
    "model.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        feats = get_penultimate_features(model, xb)\n",
    "        feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "        train_features.append(feats)\n",
    "        train_labels.append(yb.cpu().numpy())\n",
    "train_features = np.concatenate(train_features)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "\n",
    "# PCA projection (2D)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embedding = pca.fit_transform(train_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(embedding[:,0], embedding[:,1], c=train_labels, cmap='tab10', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.title('PCA projection of train set features')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
