{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist ein LSTM-Modell und wie funktioniert es?\n",
    "\n",
    "Ein **Long Short-Term Memory (LSTM)** Netzwerk ist eine spezielle Art von rekurrenten neuronalen Netzwerken (RNNs), die besonders gut für die Verarbeitung von Sequenzdaten geeignet ist. Im Gegensatz zu klassischen RNNs können LSTMs langfristige Abhängigkeiten in Daten lernen und das Problem des verschwindenden Gradienten (Vanishing Gradient Problem) lösen. Das macht LSTMs sehr leistungsfähig für Aufgaben wie Zeitreihenanalyse, Textverarbeitung und andere sequenzielle Klassifikationsaufgaben.\n",
    "\n",
    "## Basic LSTM-Architektur\n",
    "\n",
    "```python\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the fully connected layer\n",
    "        fc_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Erklärung der Basic LSTM-Architektur:**\n",
    "- Die LSTM-Schicht (`nn.LSTM`) verarbeitet die Eingabesequenz und lernt zeitliche Abhängigkeiten. Die Parameter `input_size`, `hidden_size` und `num_layers` bestimmen die Eingabedimension, die Grösse der versteckten Zustände und die Anzahl der LSTM-Schichten.\n",
    "- **Bidirektionale Option**: Wenn `bidirectional=True`, verarbeitet das LSTM die Sequenz in beide Richtungen (vorwärts und rückwärts), was die Kontextinformationen verbessert.\n",
    "- **Dropout-Regularisierung**: Wird nur angewendet, wenn mehr als eine Schicht vorhanden ist, um Overfitting zu vermeiden.\n",
    "- **Vollverbundene Schichten**: Am Ende werden die LSTM-Ausgaben durch zwei lineare Schichten auf die gewünschte Anzahl von Klassen abgebildet.\n",
    "- **Letzter Zeitschritt**: Das Modell verwendet nur den letzten Zeitschritt (`out[:, -1, :]`) für die finale Klassifikation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced LSTM mit Attention-Mechanismus\n",
    "\n",
    "```python\n",
    "class AdvancedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the fully connected layer\n",
    "        fc_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Verbesserungen des Advanced LSTM:**\n",
    "- **Attention-Mechanismus**: Statt nur den letzten Zeitschritt zu verwenden, berechnet das Modell Aufmerksamkeitsgewichte für alle Zeitschritte. Dadurch kann es automatisch die wichtigsten Teile der Sequenz identifizieren und fokussieren.\n",
    "- **Gewichteter Kontext**: Der finale Kontext-Vektor wird als gewichteter Durchschnitt aller LSTM-Ausgaben berechnet, wodurch Informationen aus der gesamten Sequenz genutzt werden.\n",
    "- **Tiefere Klassifikationsschicht**: Drei vollverbundene Schichten mit abnehmender Grösse ermöglichen komplexere Entscheidungsgrenzen.\n",
    "- **Bessere Informationsnutzung**: Alle Zeitschritte tragen zur finalen Vorhersage bei, nicht nur der letzte.\n",
    "\n",
    "---\n",
    "\n",
    "## Warum haben wir diese \"Tweaks\" verwendet?\n",
    "\n",
    "- **Bidirektionale Verarbeitung**: Ermöglicht dem Modell, sowohl vergangene als auch zukünftige Kontextinformationen zu nutzen, was besonders bei Sequenzklassifikation vorteilhaft ist.\n",
    "- **Batch First (`batch_first=True`)**: Erleichtert die Handhabung der Tensoren, da die Batch-Dimension an erster Stelle steht.\n",
    "- **Adaptive Dropout**: Dropout wird nur bei mehrschichtigen LSTMs angewendet, da es bei einschichtigen LSTMs zu Informationsverlust führen kann.\n",
    "- **Attention-Mechanismus**: Löst das Problem, dass bei langen Sequenzen wichtige Informationen vom Anfang der Sequenz verloren gehen können.\n",
    "- **Graduelle Dimensionsreduktion**: In der Advanced-Version werden die Dimensionen schrittweise reduziert (hidden_size → hidden_size//2 → num_classes), was zu stabilerem Training führt.\n",
    "- **Hyperparameter-Tuning**: Parameter wie `hidden_size`, `num_layers`, `dropout` und `bidirectional` können im Grid Search optimiert werden, um die beste Modellkonfiguration für spezifische Datensätze zu finden.\n",
    "\n",
    "Insgesamt sorgen diese Anpassungen dafür, dass die LSTM-Modelle sowohl leistungsfähig als auch robust gegenüber verschiedenen Arten von Sequenzdaten sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist ein MLP-Modell und wie funktioniert es?\n",
    "\n",
    "Ein **Multi-Layer Perceptron (MLP)** ist ein feedforward-Netzwerk, das aus mehreren vollverbundenen Schichten (Dense Layers) besteht. Im Gegensatz zu CNNs oder LSTMs hat ein MLP keine spezielle Struktur für räumliche oder zeitliche Daten, sondern verarbeitet die Eingaben als flache Vektoren. MLPs sind sehr vielseitig und eignen sich hervorragend für tabellarische Daten, allgemeine Klassifikationsaufgaben und als finale Klassifikationsschichten in komplexeren Architekturen.\n",
    "\n",
    "## Basic MLP-Architektur\n",
    "\n",
    "```python\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "```\n",
    "\n",
    "**Erklärung der Basic MLP-Architektur:**\n",
    "- **Flexible Schichtgrössen**: Die Liste `hidden_sizes` bestimmt die Anzahl und Grösse der versteckten Schichten. Zum Beispiel `[128, 64, 32]` erstellt drei versteckte Schichten mit abnehmender Grösse.\n",
    "- **Lineare Transformationen**: Jede Schicht führt eine lineare Transformation (`nn.Linear`) durch, die die Eingaben mit Gewichten multipliziert und Bias addiert.\n",
    "- **Batch Normalization**: Normalisiert die Aktivierungen für stabileres und schnelleres Training.\n",
    "- **ReLU-Aktivierung**: Die Rectified Linear Unit-Funktion führt Nicht-Linearität ein und hilft beim Gradientenfluss.\n",
    "- **Dropout-Regularisierung**: Reduziert Overfitting durch zufälliges \"Ausschalten\" von Neuronen während des Trainings.\n",
    "- **Modularer Aufbau**: Die Schichten werden dynamisch basierend auf der `hidden_sizes`-Liste erstellt.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced MLP mit Residual Connections\n",
    "\n",
    "```python\n",
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.BatchNorm1d(hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(hidden_sizes[i], hidden_sizes[i+1], dropout)\n",
    "            )\n",
    "        \n",
    "        # Output layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[-1], num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        )\n",
    "        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Verbesserungen des Advanced MLP:**\n",
    "- **Residual Connections (Skip Connections)**: Ermöglichen das Training tieferer Netzwerke durch Umgehung des Vanishing Gradient Problems. Die Eingabe wird direkt zur Ausgabe addiert.\n",
    "- **Modularer Aufbau**: Getrennte Feature-Extraktion und Residual Blocks für bessere Strukturierung und Verständlichkeit.\n",
    "- **ResidualBlock-Klasse**: Implementiert das bewährte Residual-Konzept aus ResNet-Architekturen für MLPs.\n",
    "- **Adaptive Shortcut-Verbindungen**: Wenn sich die Dimensionen zwischen Eingabe und Ausgabe ändern, wird eine lineare Transformation verwendet, ansonsten eine Identity-Funktion.\n",
    "- **Doppelte Transformation**: Jeder Residual Block führt zwei lineare Transformationen durch, bevor die Residual-Verbindung addiert wird.\n",
    "\n",
    "---\n",
    "\n",
    "## Warum haben wir diese \"Tweaks\" verwendet?\n",
    "\n",
    "- **Batch Normalization**: Stabilisiert das Training durch Normalisierung der Aktivierungen, reduziert die interne Kovariatenverschiebung und ermöglicht höhere Lernraten.\n",
    "- **ReLU-Aktivierung**: Löst das Problem verschwindender Gradienten besser als Sigmoid oder Tanh-Funktionen und ist computationally effizient.\n",
    "- **Dropout-Regularisierung**: Verhindert Overfitting durch zufälliges Deaktivieren von Neuronen, was das Modell zwingt, robustere Features zu lernen.\n",
    "- **Flexible Architektur**: Die `hidden_sizes`-Liste ermöglicht einfache Anpassung der Netzwerktiefe und -breite für verschiedene Problemgrössen.\n",
    "- **Residual Connections**: Ermöglichen das Training sehr tiefer Netzwerke und verbessern den Gradientenfluss, was zu besserer Konvergenz führt.\n",
    "- **Sequential-Aufbau**: Durch `nn.Sequential` wird der Code sauberer und die Architektur ist einfacher zu verstehen und zu modifizieren.\n",
    "- **Hyperparameter-Tuning**: Parameter wie `hidden_sizes`, `dropout`, Lernrate und Batch-Grösse können im Grid Search optimiert werden.\n",
    "\n",
    "## ResidualBlock im Detail\n",
    "\n",
    "Der ResidualBlock implementiert die Formel: **F(x) = H(x) + x**, wobei:\n",
    "- **H(x)** die Transformation durch die beiden linearen Schichten ist\n",
    "- **x** die ursprüngliche Eingabe (Residual/Skip Connection)\n",
    "- **F(x)** die finale Ausgabe nach Addition und Aktivierung\n",
    "\n",
    "Dies ermöglicht es dem Netzwerk, Identitätsfunktionen leichter zu lernen und tiefere Architekturen zu trainieren.\n",
    "Insgesamt sorgen diese Anpassungen dafür, dass die MLP-Modelle sowohl für einfache als auch für komplexe Klassifikationsaufgaben optimal geeignet sind."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
